# -*- coding: utf-8 -*-
"""VirusModeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bYWWsNTHkHJ99ti1IsOCG2EAzVWgX7VY
"""

!pip install statsmodels --upgrade

"""# Coronavirus Modeling"""

import pandas as pd
import seaborn as sns;
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates

"""We get our data from [here](https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv).

The Italy Presidenza del Consiglio dei Ministri - Dipartimento della Protezione Civile Covid-19 dataset is updated daily.
"""

data = pd.read_csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-andamento-nazionale/dpc-covid19-ita-andamento-nazionale.csv")

"""## Preprocessing and cleaning

Data columns are in Italian. Have to translate them to see what they mean.
"""

data.columns

italian=['data', 'stato', 'ricoverati_con_sintomi', 'terapia_intensiva',
       'totale_ospedalizzati', 'isolamento_domiciliare', 'totale_positivi',
       'variazione_totale_positivi', 'nuovi_positivi', 'dimessi_guariti',
       'deceduti', 'totale_casi', 'tamponi', 'casi_testati', 'note_it',
       'note_en']
# from Google Translate
english=['date', 'state', 'hospitalized_with_symptoms', 'intensive_care',
       'total_hospitalized', 'home_insulation', 'total_positive',
       'total_positive_change', 'new_positives', 'discharged_healed',
       'deceased', 'total_cases', 'swabs', 'test_cases', 'note_it',
       'Note_en']

dict = {x[0]:x[1] for x in zip(italian, english)}

data = data.rename(columns=dict)

data.columns

"""Let's see if we can make some more features. One important variable is the daily change. I.e., daily new cases, deaths, recovered, etc. 

We already have daily change in positive cases as well as daily new cases. We will construct similar variables for deaths, tests, and recovered.
"""

data['new_deceased'] = data['deceased'].diff()
data['new_swabs'] = data['swabs'].diff()
data['new_healed'] = data['discharged_healed'].diff()
# have new_positives

# format date from string to timestamp
data['date_formatted'] = [pd.to_datetime(d) for d in data['date']]
data['date_formatted'].head

"""We add rolling average to smooth out weekends. We know that weekend tests are lower, which result in a weekly cyclical behavior."""

rolling_weekly_average = 7
data['new_positives_rolling'] = data['new_positives'].rolling(window=rolling_weekly_average).mean()
data['new_healed_rolling'] = data['new_healed'].rolling(window=rolling_weekly_average).mean()
data['new_swabs_rolling'] = data['new_swabs'].rolling(window=rolling_weekly_average).mean()
data['new_deceased_rolling'] = data['new_deceased'].rolling(window=rolling_weekly_average).mean()

"""We categorize intense cases as in intensive care or deceased."""

data['intense_cases'] = data['new_deceased'] + data['intensive_care']

"""## Visualization"""

# correlation heatmap
fig, ax = plt.subplots(figsize=(15, 8), facecolor='white')
ax = sns.heatmap(data.corr(), cmap="YlGnBu", ax=ax)

def plotter(col='new_positives', data=data, mark_max=True):
    '''
    Plots column from data

    mark_max: if true, plot vertical line marking when the max value 
    occurs for col
    '''
    fig, ax = plt.subplots(figsize=(12, 9), facecolor='white')
    ax.grid()
    ax.spines["top"].set_visible(False)  
    ax.spines["right"].set_visible(False) 

    # plot data
    ax.scatter(data['date_formatted'], data[col], color='tab:blue')

    ax.set(xlabel='Date', ylabel=col, title='Italy '+col)
    ax.set_facecolor('white')

    ax.xaxis.set_major_formatter(DateFormatter('%m-%d'))
    ax.xaxis.set_major_locator(mdates.DayLocator(interval = 3))

    if mark_max:
        # find date with max value of col
        date = data.loc[data[col].idxmax()]['date_formatted']
        ax.axvline(date, c="tab:red", zorder=0)
    fig.savefig('italy_'+ col + '.png')
    plt.show()

plotter('new_positives')

plotter('new_positives_rolling')

plotter('new_deceased')

plotter('new_deceased_rolling')

plotter('new_swabs')

plotter('new_swabs_rolling')

plotter('new_healed')

plotter('new_healed_rolling')

plotter('intensive_care')

plotter('total_cases',mark_max=False)

plotter('intense_cases')

"""Plot percentage to see what the positive rates are."""

def percentage_plotter(num_col='new_positives', denom_col='new_swabs', 
                       data=data, mark_max=True):
    '''
    Plots percentage from data calculated as num_col/denom_col * 100

    mark_max: if true, plot vertical line marking when the max value 
    occurs for col
    '''
    perc_d = 100*(data[num_col]/data[denom_col])
    fig, ax = plt.subplots(figsize=(12, 9), facecolor='white')
    ax.grid()
    ax.scatter(data['date_formatted'], perc_d, color='tab:blue')
    ax.set(xlabel='Date',ylabel='percent '+num_col,title='Italy '+'percent '+num_col)
    ax.set_facecolor('white')
    ax.xaxis.set_major_formatter(DateFormatter('%m-%d'))
    ax.xaxis.set_major_locator(mdates.DayLocator(interval = 3))

    if mark_max:
        # find date with max value of col
        date = data.loc[perc_d.idxmax()]['date_formatted']
        ax.axvline(date, c="tab:red", zorder=0)

    fig.savefig('italy_'+'percent '+num_col+'.png')
    plt.show()

percentage_plotter()

percentage_plotter('new_positives_rolling','new_swabs_rolling')

percentage_plotter('intense_cases','total_positive')

"""## Modeling

We can build our model using supervised learning, using regression models to predict the next `y` variable. 

Since this is a time series data, by repeatedly predicting `y` for the following day, we can have the model give us a sense of the disease progression in the near future. Obviously, as we are refeeding predicted `y` into our model to predict the subsequent value in our time series, the model will get less accurate the further it predicts into the future.

For our `y` target, we will try both `intense_cases` as well as `new_positives`.

### Naive Supervised
Given `X` we predict `y`

Make train test split data. We divide target y column into train and test sets to fit regression.
"""

from sklearn import linear_model
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.linear_model import Lasso, LassoCV
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn.model_selection import GridSearchCV

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

from sklearn.metrics import max_error
import math

# Params
y_col='intense_cases'
train_test_split = 0.7

def get_train_test(perc=0.5, y_col = 'intense_cases'):
    ''' 
    returns train test split of column y_col 
    perc is train data percentage
    '''
    y = data[y_col].fillna(0)
    total_days = len(y)
    x=np.array([i for i in range(len(y))])

    test_proportion = int(perc*total_days)
    x_train, y_train = x[:test_proportion], y[:test_proportion]
    x_test, y_test = x[test_proportion:], y[test_proportion:]

    return x_train.reshape(-1, 1), y_train.values.reshape(-1, 1), x_test.reshape(-1, 1), y_test.values.reshape(-1, 1), test_proportion

x_train, y_train, x_test, y_test, cutoff_idx = get_train_test(perc=0.67, y_col = y_col)

def trainer(model, train_test_split=train_test_split, y_col=y_col):
    '''
    Function to train model using a percentage of observed data and predict the rest
    '''
    x_train, y_train, x_test, y_test, cutoff_idx = get_train_test(perc=0.67, y_col = y_col)
    model.fit(x_train, y_train)
    print("Fitting Complete.\nR Score is: %s" %model.score(x_train, y_train))
    y_pred = model.predict(x_train)
    error = max_error(y_train, y_pred)
    print("max error is %s" %(error))

    y_pred = model.predict(x_test)
    y_pred_max = [y + error for y in y_pred]
    y_pred_min = [y - error for y in y_pred] 

    # plot known data
    plt.grid()
    plt.scatter(data['date_formatted'], data[y_col])

    x_date = data['date_formatted'][cutoff_idx:]

    # plot linear regression prediction
    plt.plot(x_date, y_pred, color='green', linewidth=2)

    # plot error intervals
    plt.plot(x_date, y_pred_max, color='red', linewidth=1, linestyle='dashed')
    plt.plot(x_date, y_pred_min, color='red', linewidth=1, linestyle='dashed')

    plt.xlabel('Date')

    plt.ylabel(y_col)
    plt.savefig(y_col + "_prediction.png")
    plt.show()
    return model, x_train, y_train, x_test, y_test, cutoff_idx

"""Linear Regression"""

linreg = linear_model.LinearRegression()

linreg, x_train, y_train, x_test, y_test, cutoff_idx = trainer(linreg)

params = {'alpha': [10**x for x in range(-10, 2, 1)]}
ridge = Ridge()
ridge_grid = GridSearchCV(ridge, params, scoring='neg_mean_squared_error', cv=5)
ridge_grid.fit(x_train, y_train)
ridge=Ridge(**ridge_grid.best_params_)

ridge, x_train, y_train, x_test, y_test, cutoff_idx = trainer(ridge)

lasso = Lasso(max_iter=1e6)
params = {'alpha': [10**x for x in range(-5, 1, 1)]}
lasso_grid = GridSearchCV(lasso, params, scoring='neg_mean_squared_error', cv=5)
lasso_grid.fit(x_train, y_train)
lasso=Lasso(**lasso_grid.best_params_)

lasso, x_train, y_train, x_test, y_test, cutoff_idx = trainer(lasso)

ratios = [0.1,0.2,0.3,0.4,0.5] + [x/100 for x in range(50, 105, 5)]
elastic_cv = ElasticNetCV(l1_ratio=ratios)
elastic_cv.fit(x_train, y_train)
elastic=ElasticNet(**lasso_grid.best_params_)

elastic, x_train, y_train, x_test, y_test, cutoff_idx = trainer(elastic)

"""Not looking very promissing. Lets try random forest.

Random Forest
"""

# Number of trees in random forest
n_estimators = n_estimators = list(range(3,10,1))

# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = list(range(1, 10, 1))
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2,3,4,5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1,2,3]
# Method of selecting samples for training each tree
bootstrap = [True, False]

random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf = RandomForestRegressor()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(x_train, y_train)
rf = RandomForestRegressor(**rf_random.best_params_)
print(rf_random.best_params_)

rf, x_train, y_train, x_test, y_test, cutoff_idx = trainer(rf)

"""It seems to be easily overfit given how sparse the data is. Overall, supervised learning isn't working well. We have to either incorporate more features or factor in autocorrelation. Since the heatmap doesn't show much correlation between features in our dataset, let's go straight to time series models that factor in previous observations when making predictions.

### Time Series Methods
Given past observed `x_i`, we predict next `x_i+1`.

#### Lag and Autocorrelation Plots
Lets confirm that there is enough correlation to justify looking into time series models

Plot lag the which is correlation of observation at time `t` and at `t-1`. 

It looks like a strong positive correlation. From the autocorrelation plot, we don't see much cyclical correlation either. It seems that every observation is strongly positively correlated with the previous observation.
"""

pd.plotting.lag_plot(data[y_col])

pd.plotting.autocorrelation_plot(data[y_col])

"""#### Models

[Reference doc](https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/)
"""

from statsmodels.tsa.ar_model import AutoReg, ar_select_order
from statsmodels.tsa.api import acf, pacf, graphics
from statsmodels.tsa.arima_model import ARMA
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

x_train, y_train, x_test, y_test, cutoff_idx = get_train_test(perc=0.67, y_col = y_col)

def get_exog_data(exog_col='swabs', cutoff_idx=cutoff_idx):
    '''
    Returns exog_data tuple containing (exog_train, exog_test) bisected at cutoff_idx
    '''
    exog_data = (data[exog_col][:cutoff_idx].values.reshape(-1, 1),data[exog_col][cutoff_idx:].values.reshape(-1, 1))
    return exog_data

def ts_trainer(model, y_train, test_length, cutoff_idx, exog=None, forcast=False):
    '''
    Function to train time series model using a percentage of observed data 
    and predict the rest.
    test_length is number of observations to predict after y_train
    exog is tuple of train and test exogenous data for SARIMAX
    '''

    model = model.fit()

    # stats models is quite buggy -- model predict functions behave differently
    try:
        if exog is None:
            y_pred = model.predict(0, len(y_train)-1)
        else:
            y_pred = model.predict(0, len(y_train)-1, exog=[exog[0]])
        error = max_error(y_train, y_pred)

    except ValueError:
        try:
            if exog is None:
                y_pred = model.predict(0, len(y_train))
            else:
                y_pred = model.predict(0, len(y_train), exog=[exog[0]])
            error = max_error(y_train, y_pred)

        except ValueError:
            # ARIMA model is fickle
            error = 0

    print("max error is %s" %(error))

    if exog is None:
        y_pred = model.predict(len(y_train), len(y_train)+test_length-1)
    else:
        y_pred = model.predict(len(y_train), len(y_train)+test_length-1, 
                               exog=[exog[1]])

    # keep values above 0
    y_pred[y_pred < 0] = 0

    y_pred_max = [y + error for y in y_pred]
    y_pred_min = [y - error for y in y_pred] 

    # plot data
    fig, ax = plt.subplots(figsize=(12, 9), facecolor='white')
    ax.grid()
    ax.spines["top"].set_visible(False)  
    ax.spines["right"].set_visible(False)  

    # plot existing data
    ax.scatter(data['date_formatted'], data[y_col], color='tab:blue')

    # use existing date time or forecasting future dates
    if 0 < cutoff_idx < len(data['date_formatted']):
        x_date = data['date_formatted'][cutoff_idx:]
    else: 
        # generate the next n days, n=test_length
        last_date = data['date_formatted'][len(data['date_formatted'])-1]
        x_date = [last_date + timedelta(days=i) for i in range(test_length)]
        x_date = np.array(x_date)

    # plot predicted values
    ax.plot(x_date, y_pred, color='white' if forcast else 'tab:green', linewidth=2)

    # plot error intervals
    if forcast:
        ax.fill_between(x_date, y_pred_max,  
                 y_pred_min, color="#3F5D7D") 
        # if reaches zero, mark on plot with vertical line
        min_index = np.where(y_pred == 0)[0]
        if len(min_index) > 0:
            date = x_date[min_index[0]]
            ax.axvline(date, c='tab:green')
    else:
        ax.plot(x_date, y_pred_max, color='tab:red', linewidth=1, linestyle='dashed')
        ax.plot(x_date, y_pred_min, color='tab:red', linewidth=1, linestyle='dashed')

    plt.title("Italy " + y_col, fontsize=22)
    plt.xlabel('Date', fontsize=16)

    plt.ylabel(y_col, fontsize=16)
    plt.savefig(y_col + "_ts_prediction.png")
    plt.show()
    return model

autoreg = AutoReg(y_train, lags=1)
autoreg = ts_trainer(autoreg, y_train, len(y_test), cutoff_idx)

ma = ARMA(y_train, order=(0, 1))
ma = ts_trainer(ma, y_train, len(y_test), cutoff_idx)

arma = ARMA(y_train, order=(1, 1))
arma = ts_trainer(arma, y_train, len(y_test), cutoff_idx)

arma = ARMA(y_train, order=(2, 1))
arma = ts_trainer(arma, y_train, len(y_test), cutoff_idx)

arima = ARIMA(y_train, order=(1, 1, 1))
arima = ts_trainer(arima, y_train, len(y_test), cutoff_idx)

arima = ARIMA(y_train, order=(1, 2, 1))
arima = ts_trainer(arima, y_train, len(y_test), cutoff_idx)

sarima = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 5))
sarima = ts_trainer(sarima, y_train, len(y_test), cutoff_idx)

sarima = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 15))
sarima = ts_trainer(sarima, y_train, len(y_test), cutoff_idx)

sarima = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 10))
sarima = ts_trainer(sarima, y_train, len(y_test), cutoff_idx)

sarima = SARIMAX(y_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))
sarima = ts_trainer(sarima, y_train, len(y_test), cutoff_idx)

sarima = SARIMAX(y_train, order=(1, 1, 2), seasonal_order=(1, 1, 1, 10))
sarima = ts_trainer(sarima, y_train, len(y_test), cutoff_idx)

"""The SARIMA model seems to be fitting the trend well when incorporating 10 day seasonal cycles. Strangely, weekly cycles are worse fits.

Next, let us try adding an exogenous variable to see what happens
"""

exog_data = get_exog_data('swabs')
sarimax = SARIMAX(y_train, exog=exog_data[0], order=(1, 1, 1), seasonal_order=(1, 1, 1, 10))
sarimax = ts_trainer(sarimax, y_train, len(y_test), cutoff_idx, exog=exog_data)

exog_data = get_exog_data('new_positives')
sarimax = SARIMAX(y_train, exog=exog_data[0], order=(1, 1, 1), seasonal_order=(1, 1, 1, 10))
sarimax = ts_trainer(sarimax, y_train, len(y_test), cutoff_idx, exog=exog_data)

"""As expected, the fit is worse, since the data features are either endogenous (don't include new information not contained in `y`) or don't correlate strongly.

Let's use the SARIMA model to predict the next 20 days.
"""

forcast_next=50
y_total=np.concatenate([y_train, y_test])
sarima = SARIMAX(y_total, order=(1, 1, 1), seasonal_order=(1, 1, 2, 10))
sarima = ts_trainer(sarima, y_total, test_length=forcast_next, cutoff_idx=-1, forcast=True)